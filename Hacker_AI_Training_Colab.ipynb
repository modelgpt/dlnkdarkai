{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Hacker AI V3 - Full Auto Training (Final Version)\n",
    "\n",
    "**Pure hacker mindset - NO limits**\n",
    "\n",
    "This notebook fine-tunes the V2 model (`dlnkgpt/dLNk-gpt-j-6b-agent-v2-phase2`) using the primary dataset `fulldata.jsonl` to create the V3 model.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ **Upgraded Base Model:** Starts from the powerful V2 model.\n",
    "- ‚úÖ **Primary Dataset:** Uses `fulldata.jsonl` directly.\n",
    "- ‚úÖ **Anti-disconnect & Auto-recovery:** Ensures stable, long-running training sessions.\n",
    "- ‚úÖ **Monitoring:** Integrated with Telegram and W&B for real-time updates.\n",
    "- ‚úÖ **Auto-upload:** Pushes the final model to HuggingFace Hub.\n",
    "\n",
    "---\n",
    "\n",
    "## üîë 1. Setup Secrets\n",
    "\n",
    "Go to the[object Object]   "source": [
    "# 2. Install Dependencies\n",
    "%pip install -q transformers[torch] datasets accelerate bitsandbytes wandb huggingface_hub requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Import Libraries & Setup Environment\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "from google.colab import userdata\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "import wandb\n",
    "\n",
    "# Get secrets\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
    "    TELEGRAM_BOT_TOKEN = userdata.get('TELEGRAM_BOT_TOKEN')\n",
    "    TELEGRAM_CHAT_ID = userdata.get('TELEGRAM_CHAT_ID')\n",
    "    print('‚úÖ Secrets loaded successfully.')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error loading secrets: {e}')\n",
    "    print('üõë Please ensure you have set up all required secrets in the Colab sidebar.')\n",
    "\n",
    "# Login to services\n",
    "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "!huggingface-cli login --token {HF_TOKEN}\n",
    "\n",
    "# Telegram notification function\n",
    "def send_telegram(message):\n",
    "    try:\n",
    "        url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\"\n",
    "        data = {\"chat_id\": TELEGRAM_CHAT_ID, \"text\": message}\n",
    "        requests.post(url, data=data, timeout=5)\n",
    "    except Exception as e:\n",
    "        print(f'Telegram notification failed: {e}')\n",
    "\n",
    "# Anti-disconnect keep-alive thread\n",
    "def keep_alive():\n",
    "    while True:\n",
    "        time.sleep(300)  # Every 5 minutes\n",
    "        print(f'\nüîÑ Keep-alive check at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "\n",
    "keep_alive_thread = threading.Thread(target=keep_alive, daemon=True)\n",
    "keep_alive_thread.start()\n",
    "\n",
    "print('\n‚úÖ Setup complete with anti-disconnect!')\n",
    "send_telegram('üî• Hacker AI V3 Training Initialized (fulldata.jsonl)!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Configuration\n",
    "CONFIG = {\n",
    "    # Model setup\n",
    "    \"base_model\": \"dlnkgpt/dLNk-gpt-j-6b-agent-v2-phase2\",  # Upgraded to V2\n",
    "    \"dataset_file": "fulldata.jsonl",  # Using the primary dataset\n",
    "    \"output_dir\": \"./hacker_ai_model_v3\",\n",
    "    \"hf_repo\": \"dlnkgpt/hacker-ai-v3\",\n",
    "    \"wandb_project\": \"Hacker-AI-V3-Training-fulldata\",\n",
    "    \n",
    "    # Training params (optimized for A100)\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_epochs\": 3,\n",
    "    \"max_length\": 2048,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"save_steps\": 500,\n",
    "    \"eval_steps\": 500,\n",
    "    \"logging_steps": 10, # Reduced from 50 to 10 based on W&B analysis to get more data points,\n",
    "}\n",
    "\n",
    "print(\"üìã Configuration set for V3 training with fulldata.jsonl:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Unzip Dataset and Load/Prepare Dataset\n# Unzip the dataset file\n!unzip -o fulldata.zip\n",
    "print(\"üìä Loading dataset...\")\n",
    "\n",
    "dataset = load_dataset('json', data_files=CONFIG[\"dataset_file\"])\n",
    "\n",
    "# Instruction-following format\n",
    "def format_sample(sample):\n",
    "    text = f\"\"\"### Instruction:\\n{sample['instruction']}\\n\\n### Input:\\n{sample['input']}\\n\\n### Response:\\n{sample['output']}\"\"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_sample, remove_columns=dataset['train'].column_names)\n",
    "\n",
    "# Split into training and validation sets\n",
    "dataset = dataset['train'].train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "print(f\"‚úÖ Train samples: {len(dataset['train'])} | Validation samples: {len(dataset['test'])}\")\n",
    "send_telegram(f\"üìä Dataset loaded: {len(dataset['train'])} train / {len(dataset['test'])} val samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Load Model and Tokenizer\n",
    "print(f\"ü§ñ Loading base model: {CONFIG['base_model']}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"base_model\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,  # Use 8-bit quantization for memory efficiency\n",
    ")\n",
    "\n",
    "# Tokenize the dataset\n",
    "print(\"üî§ Tokenizing dataset...\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model, tokenizer, and dataset are ready!\")\n",
    "send_telegram(f\"ü§ñ Model {CONFIG['base_model']} loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Configure Training\n",
    "\n",
    "# Initialize W&B monitoring\n",
    "wandb.init(\n",
    "    project=CONFIG[\"wandb_project\"],\n",
    "    name=f\"hacker-ai-v3-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "# Set up Training Arguments (cleaned up)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    \n",
    "    # Evaluation and Saving Strategy\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Logging\n"    # Logging (logging_steps reduced from 50 to 10 for better metric granularity)\n    logging_steps=CONFIG[\"logging_steps\"],\n    report_to=\"wandb\",\n",
    "    \n",
    "    # Optimization\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    # Overfitting Prevention\n",
    "    weight_decay=0.01,          # L2 regularization\n",
    "    max_grad_norm=1.0,          # Gradient clipping\n",
    "    lr_scheduler_type=\"cosine\", # Learning rate scheduling\n",
    "    \n",
    "    # Other\n",
    "    push_to_hub=False,  # We will push manually at the end\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "# Note: The custom OverfittingAwareTrainer has been removed for simplicity and stability.\n",
    "# The standard Trainer with load_best_model_at_end=True provides robust early stopping.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Start Training\n",
    "print(\"\"\"\n",
    "===================================================\n",
    "üî• STARTING HACKER AI V3 TRAINING (from V2) üî•\n",
    "===================================================\n",
    "\"\"\")\n",
    "send_telegram(\"üöÄ Training is now starting! Check W&B for live progress.\")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    training_duration = str(datetime.now() - wandb.run.start_time)\n",
    "    print(f\"üéâ Training complete! Duration: {training_duration}\")\n",
    "    send_telegram(f\"üéâ Training complete! Duration: {training_duration}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    error_trace = traceback.format_exc()\n",
    "    print(f'\n‚ùå An error occurred during training: {e}')\n",
    "    print(error_trace)\n",
    "    send_telegram(f'‚ùå TRAINING ERROR: {str(e)[:500]}')\n",
    "    # Attempt to save a recovery checkpoint\n",
    "    try:\n",
    "        print('\nüîÑ Attempting to save recovery checkpoint...')\n",
    "        trainer.save_model(f\"{CONFIG['output_dir']}_recovery\")\n",
    "        print('‚úÖ Recovery checkpoint saved!')\n",
    "        send_telegram('‚úÖ Recovery checkpoint saved!')\n",
    "    except Exception as save_e:\n",
    "        print(f'‚ùå Auto-recovery failed: {save_e}')\n",
    "finally:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Save and Upload Final Model\n",
    "print(\"üíæ Saving final model locally...\")\n",
    "trainer.save_model(CONFIG[\"output_dir\"])\n",
    "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
    "print(f\"‚úÖ Model saved to {CONFIG['output_dir']}\")\n",
    "send_telegram(\"üíæ Model saved locally!\")\n",
    "\n",
    "print(f\"üì§ Uploading to HuggingFace Hub: {CONFIG['hf_repo']}...\")\n",
    "try:\n",
    "    api = HfApi()\n",
    "    api.upload_folder(\n",
    "        folder_path=CONFIG[\"output_dir\"],\n",
    "        repo_id=CONFIG[\"hf_repo\"],\n",
    "        repo_type=\"model\",\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    print(\"‚úÖ Uploaded to HuggingFace!\")\n",
    "    send_telegram(f\"‚úÖ Model uploaded: https://huggingface.co/{CONFIG['hf_repo']}\")\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Upload error: {e}')\n",
    "    send_telegram(f'‚ùå Upload error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéâ Training Complete!\n",
    "\n",
    "Your **Hacker AI V3** model is ready!\n",
    "\n",
    "**Model location:**\n",
    "- Local: `./hacker_ai_model_v3/`\n",
    "- HuggingFace: `https://huggingface.co/dlnkgpt/hacker-ai-v3`\n",
    "\n",
    "You can now use this model for inference."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
